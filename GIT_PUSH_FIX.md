# Git Push Fix Guide

## ðŸš¨ Problem
Your repository is **707 MB** - too large for GitHub!

## ðŸ’¡ Solution

### Step 1: Stop Tracking Large Files

I've updated `.gitignore` to exclude:
- âœ… `artifacts/` (model files ~500+ MB)
- âœ… `data/` (CSV datasets ~100+ MB)
- âœ… `venv/` (virtual environment ~100+ MB)
- âœ… `logs/` (log files)
- âœ… `__pycache__/` (Python cache)

### Step 2: Remove Already Tracked Files

Run these commands:

```bash
# Remove large files from Git (but keep them locally)
git rm -r --cached artifacts/
git rm -r --cached data/
git rm -r --cached venv/
git rm -r --cached logs/
git rm -r --cached legacy_backup/

# Find and remove all .pkl files
git rm --cached **/*.pkl

# Find and remove all .csv files  
git rm --cached **/*.csv
```

### Step 3: Commit the Changes

```bash
git add .gitignore
git commit -m "Remove large files from Git tracking"
```

### Step 4: Increase Buffer Size & Push

```bash
# Increase buffer size
git config http.postBuffer 524288000

# Push with larger timeout
git push -u origin main
```

## ðŸ“‹ Recommended: Create a Download Script

Instead of committing large files, create a script to download/generate them:

**`download_data.py`:**
```python
# Instructions to download the dataset
print("Download fraud dataset from:")
print("https://www.kaggle.com/datasets/...")
```

**`artifacts/README.md`:**
```markdown
# Model Artifacts

These files are generated by running:
1. python src/components/data_ingestion.py
2. python src/components/data_transformation.py
3. python src/components/model_training.py

Not included in Git due to size.
```

## ðŸŽ¯ Quick Fix Commands

```bash
# 1. Remove cached large files
git rm -r --cached artifacts/ data/ venv/ logs/ legacy_backup/

# 2. Commit
git add .
git commit -m "Remove large files and update .gitignore"

# 3. Increase buffer
git config http.postBuffer 524288000

# 4. Push
git push -u origin main
```

## âœ… After This

Your repository should be < 50 MB and push successfully!

## ðŸ“ For Team Members

Add to README.md:
```markdown
## Setup

1. Clone the repo
2. Download data: [link to Kaggle/Drive]
3. pip install -r requirements.txt
4. Train model: run training pipeline
```
